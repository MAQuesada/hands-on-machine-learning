{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transfer Learning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). \n",
    "\n",
    "Como estamos utilizando capas `Densas`, sólo se pueden reutilizar los patrones que aparecen en el mismo lugar (por el contrario, las capas convolucionales transferirán mucho mejor, ya que los patrones aprendidos se pueden detectar en cualquier lugar de la imagen, como veremos en el capítulo sobre CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6)  # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2  # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "\n",
    "    # binary classification task: is it a shirt (class 6)?\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32)\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43986, 28, 28), (200, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape, X_train_B.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, BatchNormalization\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(Dense(n_hidden, activation=\"selu\", kernel_initializer='lecun_normal'))\n",
    "model_A.add(Dense(8, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.5761 - accuracy: 0.8216 - val_loss: 0.3905 - val_accuracy: 0.8675\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.3528 - accuracy: 0.8803 - val_loss: 0.3252 - val_accuracy: 0.8857\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.3156 - accuracy: 0.8916 - val_loss: 0.2968 - val_accuracy: 0.9003\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2972 - accuracy: 0.8984 - val_loss: 0.2841 - val_accuracy: 0.9036\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2845 - accuracy: 0.9019 - val_loss: 0.2751 - val_accuracy: 0.9068\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2751 - accuracy: 0.9059 - val_loss: 0.2702 - val_accuracy: 0.9081\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2671 - accuracy: 0.9085 - val_loss: 0.2697 - val_accuracy: 0.9088\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2606 - accuracy: 0.9109 - val_loss: 0.2623 - val_accuracy: 0.9098\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2555 - accuracy: 0.9121 - val_loss: 0.2574 - val_accuracy: 0.9131\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2505 - accuracy: 0.9147 - val_loss: 0.2532 - val_accuracy: 0.9153\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2462 - accuracy: 0.9161 - val_loss: 0.2508 - val_accuracy: 0.9158\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2424 - accuracy: 0.9173 - val_loss: 0.2523 - val_accuracy: 0.9136\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2392 - accuracy: 0.9185 - val_loss: 0.2459 - val_accuracy: 0.9158\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 8s 5ms/step - loss: 0.2358 - accuracy: 0.9191 - val_loss: 0.2430 - val_accuracy: 0.9200\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2329 - accuracy: 0.9199 - val_loss: 0.2503 - val_accuracy: 0.9138\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2298 - accuracy: 0.9214 - val_loss: 0.2419 - val_accuracy: 0.9183\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 8s 6ms/step - loss: 0.2274 - accuracy: 0.9222 - val_loss: 0.2512 - val_accuracy: 0.9131\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2246 - accuracy: 0.9224 - val_loss: 0.2486 - val_accuracy: 0.9118\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2222 - accuracy: 0.9242 - val_loss: 0.2368 - val_accuracy: 0.9180\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 7s 5ms/step - loss: 0.2200 - accuracy: 0.9247 - val_loss: 0.2386 - val_accuracy: 0.9180\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                      validation_data=(X_valid_A, y_valid_A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"resources/my_model_A.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing pretrined inside layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"resources/my_model_A.h5\")\n",
    "\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `model_B_on_A` and `model_A` actually share layers now, so when we train one, it will update both models. If we want to avoid that, we need to build `model_B_on_A` on top of a **clone** of `model_A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 2s 121ms/step - loss: 0.4481 - accuracy: 0.7750 - val_loss: 0.4466 - val_accuracy: 0.7718\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.4164 - accuracy: 0.8350 - val_loss: 0.4175 - val_accuracy: 0.8185\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.3886 - accuracy: 0.8800 - val_loss: 0.3914 - val_accuracy: 0.8519\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.3637 - accuracy: 0.9050 - val_loss: 0.3687 - val_accuracy: 0.8803\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego descongelar las capas reutilizadas (lo que requiere compilar el modelo de\n",
    "nuevo) y continuar el entrenamiento para afinar las capas reutilizadas. Después de descongelar las capas reutilizadas, suele ser una buena idea reducir la tasa de aprendizaje, una vez más para evitar\n",
    "dañar los pesos reutilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 2s 100ms/step - loss: 0.3438 - accuracy: 0.9300 - val_loss: 0.3520 - val_accuracy: 0.8996\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3281 - accuracy: 0.9350 - val_loss: 0.3372 - val_accuracy: 0.9189\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3142 - accuracy: 0.9500 - val_loss: 0.3239 - val_accuracy: 0.9300\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3017 - accuracy: 0.9500 - val_loss: 0.3120 - val_accuracy: 0.9391\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.2905 - accuracy: 0.9500 - val_loss: 0.3011 - val_accuracy: 0.9473\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.2801 - accuracy: 0.9550 - val_loss: 0.2910 - val_accuracy: 0.9513\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.2706 - accuracy: 0.9650 - val_loss: 0.2814 - val_accuracy: 0.9554\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.2615 - accuracy: 0.9650 - val_loss: 0.2727 - val_accuracy: 0.9574\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.2533 - accuracy: 0.9700 - val_loss: 0.2653 - val_accuracy: 0.9594\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.2462 - accuracy: 0.9750 - val_loss: 0.2577 - val_accuracy: 0.9594\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2389 - accuracy: 0.9800 - val_loss: 0.2506 - val_accuracy: 0.9625\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.2322 - accuracy: 0.9800 - val_loss: 0.2444 - val_accuracy: 0.9645\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2262 - accuracy: 0.9800 - val_loss: 0.2382 - val_accuracy: 0.9655\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2203 - accuracy: 0.9800 - val_loss: 0.2325 - val_accuracy: 0.9716\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2148 - accuracy: 0.9800 - val_loss: 0.2276 - val_accuracy: 0.9726\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.2100 - accuracy: 0.9800 - val_loss: 0.2226 - val_accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-4),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deberias estar convencido con el resultado, ya que el autor hizo trmapas: *Probé muchas configuraciones hasta que encontré una que demostraba una gran mejora. Si intenta cambiar las clases o la semilla aleatoria, verá que la mejora suele disminuir, o incluso desaparecer o invertirse.*\n",
    "\n",
    "¿Por qué hice trampa? Resulta que el aprendizaje por transferencia no\n",
    "funciona muy bien con redes densas pequeñas, presumiblemente porque\n",
    "las redes pequeñas aprenden n pocos patrones, y las redes densas\n",
    "aprenden patrones muy específicos, que probablemente no sean útiles en\n",
    "otras tareas. El aprendizaje por transferencia funciona mejor con redes\n",
    "neuronales convolucionales profundas, que tienden a aprender detectores\n",
    "de características que son mucho más generales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Faster Optimizers**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Momentum optimization**\n",
    "```python \n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9) \n",
    "```\n",
    "* **Nesterov Accelerated Gradient**\n",
    "```python \n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "```\n",
    "* **AdaGrad** \n",
    "```python \n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "```\n",
    "* **RMSProp**\n",
    "```python \n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "```\n",
    "* **Adam Optimization** \n",
    "```python \n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "* **AdaMax Optimization** \n",
    "```python \n",
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "* **Nadam Optimization** \n",
    "```python \n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
