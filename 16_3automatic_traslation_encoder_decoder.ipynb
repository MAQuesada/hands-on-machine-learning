{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Automatic Translation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dado que **regular tensors**  tienen formas\n",
    "fijas, solo pueden contener oraciones de la misma longitud. Puede usar **masking** para manejar esto (es decir que ignore los tokens de relleno-pad). Sin\n",
    "embargo, si las oraciones tienen longitudes muy diferentes, no puede\n",
    "simplemente recortarlas como hicimos para sentiment analysis.   En su lugar, agrupe las oraciones en buckets(cubos) de longitud similar(e.g., a bucket for the 1- to 6-word\n",
    "sentences, another for the 7- to 12-word sentences, and so on),\n",
    "usando pad(relleno) para las secuencias más cortas para asegurarse de que todas las\n",
    "oraciones en un bucket tienen la misma longitud\n",
    "(consulte la función `tf.data.experimental.bucket_by_sequence_length()` para\n",
    "esto).\n",
    "\n",
    "* Queremos ignorar cualquier salida más allá del token EOS, por lo que estos\n",
    "tokens no deberían contribuir a la pérdida (deben enmascararse)\n",
    "\n",
    "* Cuando el vocabulario de salida es grande , generar\n",
    "una probabilidad para cada una de las palabras  sería\n",
    "terriblemente lento.  Para evitar esto, una solución es mirar solo los logits generados por \n",
    "el modelo para encontrar la palabra correcta y una muestra aleatoria de palabras incorrectas, luego\n",
    "calcular una aproximación de la pérdida basada solo en estos logits. Esta tecnica es llamada` sampled softmax` y en tensorflow  se puede usar `tf.nn.sampled_softmax_loss()` durante el entrenamiento pero la funcion sofmax normal durante la inferencia (sampled softmax cannot be used at inference time because it\n",
    "requires knowing the target)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proyecto `TensorFlow Addons` incluye muchas herramientas de *sequence-to-sequence** que le permiten crear fácilmente **encoder-decoder** listos\n",
    "para la producción.  TFA ya no se desarrollará activamente, seguirá recibiendo soporte mínimo para corrección de errores y actualizaciones de seguridad hasta su fin de vida en mayo de 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "vocab_size = 100\n",
    "embed_size = 10\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "# return_state=True para que podamos obtener su estado oculto final y pasarlo al decoder\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "\n",
    "# returns two hidden states (short term and long term)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "\n",
    "# The TrainingSampler role is to tell the decoder at each step what it\n",
    "# should pretend the previous output was\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "#  basic LSTMcell\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 23s 202ms/step - loss: 4.6056\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 4.6035\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "\n",
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Recurrent Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bidirectional layer will create a clone of the GRU layer (but in the reverse\n",
    "direction), and it will run both and concatenate their outputs. So although the GRU\n",
    "layer has 10 units, the Bidirectional layer will output 20 values per time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, None, 10)          660       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 20)         1320      \n",
      " l)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,980\n",
      "Trainable params: 1,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bean Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo utilizado en la generación de texto con RNN que mantiene un conjunto de posibles secuencias de salida en cada paso, eligiendo las k palabras más probables en cada paso. Se evalúa cada secuencia de salida utilizando una función de puntuación y se selecciona la secuencia de salida con la puntuación más alta. El Beam Search se utiliza para mejorar la calidad de la generación de texto y para evitar la selección de soluciones subóptimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "start_tokens = np.full([1000], 1)  # Utiliza el token 1 como inicio de la secuencia de salida\n",
    "end_token = np.full([1000], 2)  # Utiliza el token 2 como fin de la secuencia de salida\n",
    "\n",
    "decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
    "    cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)\n",
    "\n",
    "decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(\n",
    "    encoder_state, multiplier=beam_width)\n",
    "outputs, _, _ = decoder(\n",
    "    decoder_embeddings, start_tokens=start_tokens, end_token=end_token,\n",
    "    initial_state=decoder_initial_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
