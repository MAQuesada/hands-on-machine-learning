{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IMDb reviews` dataset is the “hello world” of natural language processing: it consists of\n",
    "50,000 movie reviews in English (25,000 for training, 25,000 for testing)You can load the IMDB dataset easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# load train and test data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "\n",
    "X_train[0][:10]\n",
    "#\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is already preprocessed for you: X_train consists of a list of reviews, each of which\n",
    "is represented as a NumPy array of integers, where each integer represents\n",
    "a word. All punctuation was removed, and then words were converted to\n",
    "lowercase, split by spaces, and finally indexed by frequency (so low\n",
    "integers correspond to frequent words). The integers 0, 1, and 2 are\n",
    "special: they represent the padding(relleno) token, the start-of-sequence (SSS)\n",
    "token, and unknown words, respectively. If you want to visualize a review,\n",
    "you can decode it like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\n",
    "\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un proyecto real, deberá preprocesar el texto usted mismo. Puede hacerlo usando la\n",
    "misma clase `Tokenizer` que usamos anteriormente, pero esta vez configurando\n",
    "char_level=False (que es el valor predeterminado). \n",
    " \n",
    "Ademas psts implementar su modelo en un dispositivo móvil o en un navegador web, y no quiere\n",
    "tener que escribir una función de preprocesamiento diferente cada vez, querrá manejar\n",
    "el preprocesamiento usando solo operaciones de TensorFlow, para que pueda incluirse\n",
    "en el modelo mismo. Veamos cómo. Primero, carguemos las reseñas originales de IMDb,\n",
    "como texto (cadenas de bytes), usando `TensorFlow Datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\codes\\ia_tf212\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "datasets.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples\n",
    "\n",
    "print(train_size, test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside.  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things in ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mostrar las dos primeras reviews\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:100], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts by truncating the reviews, keeping only the first 300 characters of\n",
    "each: this will speed up training, and it won’t impact performance too\n",
    "much because you can generally tell whether a review is positive or not in\n",
    "the first sentence or two. Then it uses regular expressions to replace <\\br\n",
    "/> tags with spaces, and to replace any characters other than letters and\n",
    "quotes with spaces. \n",
    " \n",
    "Finally, the preprocess() function splits the\n",
    "reviews by the spaces, which returns a ragged tensor, and it converts this\n",
    "ragged tensor to a dense tensor, padding all reviews with the padding\n",
    "token \"<pad>\" so that they all have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
    "\n",
    "# ejemplo de como quedan las palabras dividivas \n",
    "preprocess(X_batch, y_batch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to construct the vocabulary. This requires going through the\n",
    "whole training set once, applying our preprocess() function, and using a\n",
    "`Counter` to count the number of occurrences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most common words:  [(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "\n",
    "print(\"the most common words: \",vocabulary.most_common()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quedarnos con las palabras mas frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "12\n",
      "11\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de tokenizacion \n",
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b\"This movie was faaaaaantastic \".split():\n",
    "    print(word_to_id.get(word) or vocab_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos agregar un paso de preprocesamiento para reemplazar cada palabra\n",
    "con su ID (es decir, su índice en el vocabulario). Al igual que hicimos en el `Capítulo 13`,\n",
    "se creara  `lookup table` for this, using 1,000 out-of-vocabulary (oov)\n",
    "buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[   22,    12,    11, 10053,     0]], dtype=int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic <pad>\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que cada batch tendra el mismo tamaño de sequencia pq se uso en la funcion process `to_tensor(default_value=b\"<pad>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera capa es una capa de incrustación, que convertirá las identificaciones de palabras\n",
    "en incrustaciones (introducidas en el Capítulo 13). La matriz de incrustación debe tener una\n",
    "fila por ID de palabra (vocab_size + num_oov_buckets) y una columna por dimensión de\n",
    "incrustación (este ejemplo usa 128 dimensiones, pero este es un hiperparámetro que puede\n",
    "ajustar). Mientras que las entradas del modelo serán tensores de forma 2D **[batch size, time steps]**, la salida de la capa\n",
    "de incrustación será un tensor de forma 3D **[batch size, time steps,embedding size]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 50s 57ms/step - loss: 0.5309 - accuracy: 0.7264\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 44s 57ms/step - loss: 0.3410 - accuracy: 0.8583\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.1802 - accuracy: 0.9367\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.1383 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 0.1115 - accuracy: 0.9590\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True,  # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "\n",
    "    keras.layers.GRU(128),  # devuelve solo la salida del último paso de tiempo.\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual masking:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo deberá aprender que las fichas de relleno `<pad>` deben\n",
    "ignorarse. ¡Pero eso ya lo sabemos! ¿Por qué no le decimos al modelo que ignore los\n",
    "tokens de relleno, para que pueda concentrarse en los datos que realmente importan? En realidad, es trivial: simplemente agregue `mask_zero=True` al crear la capa Embedding(incrustación).\n",
    "Esto significa que todas las capas descendentes ignorarán los tokens de relleno (cuyo ID es\n",
    "0)\n",
    " \n",
    " The way this works is that the Embedding layer creates a mask tensor\n",
    "equal to `K.not_equal(inputs, 0)` *(where K = keras.backend)*: it is a\n",
    "Boolean tensor with the same shape as the inputs, and it is equal to False\n",
    "anywhere the word IDs are 0, or True other wise. This mask tensor is then\n",
    "automatically propagated by the model to all subsequent layers, as long as\n",
    "the time dimension is preserved. \n",
    "\n",
    "Entonces, en este ejemplo, ambas capas GRU recibirán esta\n",
    "máscara automáticamente, pero dado que la segunda capa GRU no devuelve secuencias\n",
    "(solo devuelve la salida del último paso de tiempo), la máscara no se transmitirá a la\n",
    "capa Densa. \n",
    "\n",
    " Each layer may handle the mask\n",
    "differently, but in general they simply ignore masked time steps (i.e., time\n",
    "steps for which the mask is False). For example, when a recurrent layer\n",
    "encounters a masked time step, it simply copies the output from the\n",
    "previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "embed_size = 128\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# history = model.fit(train_set, epochs=5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Después de entrenar durante algunas épocas, el modelo será bastante bueno para\n",
    "juzgar si una review es positiva o no. Si usa TensorBoard()\n",
    "callback,puede visualizar las incrustaciones en TensorBoard a medida que se aprenden: es\n",
    "fascinante ver palabras como *\"impresionante\"* y *\"asombroso\"* agruparse\n",
    "gradualmente en un lado del espacio de incrustación, mientras que palabras como *\"\n",
    "horrible\"* y *\"terrible\"* grupo en el otro lado. Algunas palabras no son\n",
    "tan positivo como cabría esperar (al menos con este modelo), como la palabra *\"bueno\"*,\n",
    "presumiblemente porque muchas críticas negativas contienen la frase *\"no bueno\"*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "The `LSTM` and `GRU` layers have an optimized implementation for GPUs, based on\n",
    "Nvidia’s cuDNN library. However, this implementation does not support `masking`. If\n",
    "your model uses a mask, then these layers will fall back to the (much slower) default\n",
    "implementation. Note that the optimized implementation also requires you to use the\n",
    "default values for several hyperparameters: **activation, recurrent_activation,\n",
    "recurrent_dropout, unroll, use_bias, and reset_after**.\n",
    "____\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reusing Pretrained Embeddings**\n",
    " \n",
    "Se puede reutilizar **Embeddings** de palabras entrenadas en algún otro corpus de texto grande (por ejemplo, artículos de\n",
    "Wikipedia). Después de\n",
    "todo, la palabra \"increíble\" generalmente tiene el mismo significado ya sea que la uses para\n",
    "hablar de películas o de cualquier otra cosa. Además, tal vez las incrustaciones\n",
    "serían útiles para el análisis de sentimientos incluso si se entrenaron en otra tarea: dado\n",
    "que palabras como \"impresionante\" y \"sorprendente\" tienen un significado similar, es\n",
    "probable que se agrupen en el espacio de incrustación incluso para otras tareas \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma predeterminada, TF Hub almacenará en caché los archivos descargados en el\n",
    "directorio temporal del sistema local. Es posible que prefiera descargarlos en un directorio\n",
    "más permanente para evitar tener que descargarlos nuevamente después de cada limpieza\n",
    "del sistema. Para ello, establezca la variable de entorno `TFHUB_CACHE_DIR` en\n",
    "el directorio de su elección (p. ej., os.environ[\"TFHUB_CACHE_DIR\"] =\"./my_tfhub_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proyecto `TensorFlow Hub` facilita la reutilización de componentes de modelos\n",
    "previamente entrenados en sus propios modelos, denominados módulos. Simplemente navegue por el repositorio de TF Hub, encuentre el que\n",
    "necesita y copie el ejemplo de código en su proyecto, y el módulo será descargado automáticamente con sus pesos entrenados. \n",
    "\n",
    "Por ejemplo, usemos el módulo de incrustación de oraciones `nnlm en dim50, versión 1`, en\n",
    "nuestro modelo de análisis de sentimiento:\n",
    "\n",
    "This particular module is a **sentence encoder**: it takes strings as input and\n",
    "encodes each one as a single vector (in this case, a 50-dimensional vector).Internamente, analiza la cadena (dividiendo palabras en espacios) e incrusta cada palabra usando\n",
    "una matriz de incrustación que fue previamente entrenada en un corpus enorme: el corpus 7B de\n",
    "Google News (¡siete mil millones de palabras!). Luego calcula la media de todas las incrustaciones\n",
    "de palabras y el resultado es la incrustación de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "\n",
    "    # By default, a hub.KerasLayer is not trainable,\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can just load the IMDb reviews dataset—no need to preprocess it\n",
    "(except for batching and prefetching)—and directly train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.5460 - accuracy: 0.7267\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.5129 - accuracy: 0.7495\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.5082 - accuracy: 0.7530\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.5047 - accuracy: 0.7533\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.5015 - accuracy: 0.7560\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
