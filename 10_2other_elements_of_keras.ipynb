{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Other elements of Keras**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the **Functional API**\n",
    "Aunque los `Sequential models` son extremadamente comunes, a veces\n",
    "es útil construir redes neuronales con topologías más complejas, o con\n",
    "múltiples entradas o salidas. Para ello, Keras ofrece la `API Funcional`,un\n",
    "ejemplo de red neuronal no secuencial es **Wide & Deep** neural\n",
    "network.\n",
    "\n",
    "  Esta arquitectura, conecta todas o parte de las\n",
    "entradas directamente a la capa de salida, lo que hace posible que la red neuronal aprenda tanto\n",
    "patrones profundos (utilizando la ruta profunda) como reglas simples (a\n",
    "través de la ruta corta). Por el contrario, un **MLP** normal obliga a que\n",
    "todos los datos fluyan a través de toda la pila de capas; así, los patrones simples de los\n",
    "datos pueden acabar distorsionados por esta secuencia de transformaciones.\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load, split and scale the California housing dataset(because SDG will be used)\n",
    " \n",
    " Para simplificar,utilizaremos la API de Scikit-Learn `fetch_california_housing()` ya que este conjunto de datos sólo contiene características numéricas (no existe la característica\n",
    "proximidad_del_océano) y no hay valores perdidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the `Wide & Deep ANN` using **API Funtional** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Input, concatenate\n",
    "from keras import Model\n",
    "from keras.backend import clear_session\n",
    "import numpy as np\n",
    "\n",
    "# reiniciar (en caso de q ya se hayan ejecutados otras capas) la genracion del # de los sequential\n",
    "clear_session()\n",
    "\n",
    "# estableces las semillas para lograr reproducir los mismos resultados\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_ = Input(shape=X_train.shape[1:])\n",
    "hidden1 = Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = concatenate([input_, hidden2])\n",
    "output = Dense(1)(concat)\n",
    "model = Model(inputs=[input_], outputs=[output])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repasemos cada línea de este código:\n",
    "* En primer lugar, se crea un objeto **Input**, se trata de\n",
    "una especificación del tipo de entrada que recibirá el modelo, incluyendo su *shape and dtype*\n",
    "* Luego se crea una capa **Dense** con 30 neuronas,\n",
    "utilizando la función de activación ReLU. Nada más crearla,\n",
    "observa que la llamamos como una función, pasándole la \n",
    "entrada. Por eso se llama API Funcional. Nótese que sólo le\n",
    "se le esta indicando a Keras cómo debe conectar las capas entre sí;\n",
    "aún no se están procesando datos reales.\n",
    "\n",
    "* A continuación creamos una segunda capa oculta, y de nuevo la\n",
    "utilizamos como función. Nótese que le pasamos la salida de la\n",
    "primera capa oculta.\n",
    "* Después se crea una capa **Concatenate**, y una vez más\n",
    "se una inmediatamente como una función, para concatenar la\n",
    "entrada y la salida de la segunda capa oculta. \n",
    "\n",
    "* La ultima capa es la de salida, con una sola neurona y\n",
    "sin función de activación, y la llamamos como una función,\n",
    "pasándole el resultado de la concatenación.\n",
    "* Finalmente, se crea un Modelo Keras, especificando qué entradas\n",
    "y salidas utilizar.\n",
    "\n",
    "Una vez que has construido el modelo Keras, todo es exactamente igual que\n",
    "antes, así que no hay necesidad de repetirlo aquí: debes compilar el\n",
    "modelo, entrenarlo, evaluarlo y usarlo para hacer predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 30)           270         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           930         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 38)           0           ['input_1[0][0]',                \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            39          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 2.0584 - accuracy: 0.0027 - val_loss: 0.7315 - val_accuracy: 0.0044\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.6249 - accuracy: 0.0029 - val_loss: 0.6326 - val_accuracy: 0.0044\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5876 - accuracy: 0.0029 - val_loss: 0.5792 - val_accuracy: 0.0044\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5580 - accuracy: 0.0029 - val_loss: 0.5248 - val_accuracy: 0.0044\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5354 - accuracy: 0.0029 - val_loss: 0.5000 - val_accuracy: 0.0044\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5141 - accuracy: 0.0029 - val_loss: 0.4893 - val_accuracy: 0.0044\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4985 - accuracy: 0.0029 - val_loss: 0.4905 - val_accuracy: 0.0044\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4843 - accuracy: 0.0029 - val_loss: 0.4515 - val_accuracy: 0.0044\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4711 - accuracy: 0.0028 - val_loss: 0.4469 - val_accuracy: 0.0044\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4605 - accuracy: 0.0028 - val_loss: 0.4461 - val_accuracy: 0.0044\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4511 - accuracy: 0.0028 - val_loss: 0.4242 - val_accuracy: 0.0044\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4430 - accuracy: 0.0028 - val_loss: 0.4271 - val_accuracy: 0.0044\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4362 - accuracy: 0.0028 - val_loss: 0.4078 - val_accuracy: 0.0044\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4305 - accuracy: 0.0027 - val_loss: 0.3997 - val_accuracy: 0.0044\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4250 - accuracy: 0.0027 - val_loss: 0.3941 - val_accuracy: 0.0044\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4204 - accuracy: 0.0027 - val_loss: 0.3923 - val_accuracy: 0.0044\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4160 - accuracy: 0.0027 - val_loss: 0.3902 - val_accuracy: 0.0044\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4131 - accuracy: 0.0027 - val_loss: 0.3870 - val_accuracy: 0.0044\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4092 - accuracy: 0.0027 - val_loss: 0.3792 - val_accuracy: 0.0044\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4066 - accuracy: 0.0027 - val_loss: 0.3885 - val_accuracy: 0.0044\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.0021\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(\n",
    "    learning_rate=1e-3), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6457891]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to send different subsets of input features through the wide or deep paths?\n",
    " \n",
    "We will send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7). Note that 3 features will go through both (features 2, 3 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = Input(shape=[5], name=\"wide_input\")\n",
    "input_B = Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = concatenate([input_A, hidden2])\n",
    "output = Dense(1, name=\"output\")(concat)\n",
    "\n",
    "model = Model(inputs=[input_A, input_B], outputs=[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " deep_input (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 30)           210         ['deep_input[0][0]']             \n",
      "                                                                                                  \n",
      " wide_input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 30)           930         ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 35)           0           ['wide_input[0][0]',             \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            36          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,176\n",
      "Trainable params: 1,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 2.3838 - val_loss: 2.1185\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.8630 - val_loss: 0.7853\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.6834 - val_loss: 0.6412\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6213 - val_loss: 0.6059\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5849 - val_loss: 0.5735\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5573 - val_loss: 0.5535\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5349 - val_loss: 0.5296\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5168 - val_loss: 0.5021\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5010 - val_loss: 0.4810\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4877 - val_loss: 0.4632\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4771 - val_loss: 0.4447\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4678 - val_loss: 0.4346\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4601 - val_loss: 0.4255\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4536 - val_loss: 0.4190\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4480 - val_loss: 0.4141\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4433 - val_loss: 0.4090\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4393 - val_loss: 0.4073\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4357 - val_loss: 0.4039\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4320 - val_loss: 0.4012\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4296 - val_loss: 0.4090\n",
      "162/162 [==============================] - 1s 2ms/step - loss: 0.4245\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EE12351F30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 170ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "# debemos pasar un par de matrices `(X_train_A, X_train_B)`: una por entrada. \n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que desee tener **multiple outputs** por multiples casos como pueden ser:\n",
    "* La tarea puede exigirlo. Por ejemplo, puede que desee localizar y\n",
    "clasificar el objeto principal de una imagen. Se trata tanto de una\n",
    "tarea de regresión (encontrar las coordenadas del centro del\n",
    "objeto, así como su anchura y altura) como de una tarea de\n",
    "clasificación.\n",
    "* Del mismo modo, puede tener varias tareas independientes\n",
    "basadas en los mismos datos. Claro que podría entrenar una red\n",
    "neuronal por tarea, pero en muchos casos obtendrá mejores\n",
    "resultados en todas las tareas entrenando una sola red neuronal\n",
    "con una salida por tarea. Esto se debe a que la red neuronal\n",
    "puede aprender características de los datos que son útiles en\n",
    "todas las tareas. Por ejemplo, puede realizar una clasificación\n",
    "multitarea de imágenes de caras, utilizando una salida para\n",
    "clasificar la expresión facial de la persona (sonriente, sorprendida,\n",
    "etc.) y otra salida para identificar si lleva gafas o no.\n",
    "\n",
    "* Otro caso de uso es como técnica de regularización (es decir, una\n",
    "restricción de entrenamiento cuyo objetivo es reducir el\n",
    "sobreajuste y mejorar así la capacidad de generalización del\n",
    "modelo). Por ejemplo, es posible que desee añadir algunas\n",
    "salidas auxiliares en una arquitectura de red neuronal para garantizar que \n",
    "la parte subyacente de la red aprenda algo útil por sí misma,\n",
    " sin depender del resto de la red.\n",
    "  \n",
    "![alt](resources/multi-outputsANN.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding an auxiliary output for regularization (builds the networks represented in *Figure 10-16*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada salida necesitará su propia función de pérdida. Por lo tanto, cuando\n",
    "*se compile* el modelo, se debe pasar una lista de pérdidas (si\n",
    "se pasa una sola pérdida, Keras asumirá que se debe usar la misma\n",
    "pérdida para todas las salidas). Por defecto, Keras calculará todas estas\n",
    "`loss` y simplemente las sumará para obtener la pérdida final utilizada\n",
    "para el entrenamiento. Como interesa  mucho más  la salida\n",
    "principal que por la salida auxiliar (ya que sólo se utiliza para la\n",
    "regularización), se debe dar a la pérdida de la salida\n",
    "principal un peso mucho mayor. Afortunadamente, es posible establecer\n",
    "todos los pesos de pérdida al compilar el modelo como se muestra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 2.1556 - main_output_loss: 1.8355 - aux_output_loss: 5.0370 - val_loss: 1.4597 - val_main_output_loss: 1.0725 - val_aux_output_loss: 4.9443\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.9820 - main_output_loss: 0.7320 - aux_output_loss: 3.2320 - val_loss: 1.0373 - val_main_output_loss: 0.6526 - val_aux_output_loss: 4.5000\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.7895 - main_output_loss: 0.6304 - aux_output_loss: 2.2212 - val_loss: 0.9979 - val_main_output_loss: 0.6089 - val_aux_output_loss: 4.4993\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7000 - main_output_loss: 0.5848 - aux_output_loss: 1.7363 - val_loss: 0.9525 - val_main_output_loss: 0.5637 - val_aux_output_loss: 4.4517\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6488 - main_output_loss: 0.5540 - aux_output_loss: 1.5019 - val_loss: 0.9024 - val_main_output_loss: 0.5357 - val_aux_output_loss: 4.2029\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6155 - main_output_loss: 0.5305 - aux_output_loss: 1.3807 - val_loss: 0.8481 - val_main_output_loss: 0.5181 - val_aux_output_loss: 3.8182\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5907 - main_output_loss: 0.5116 - aux_output_loss: 1.3029 - val_loss: 0.7963 - val_main_output_loss: 0.5133 - val_aux_output_loss: 3.3431\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5720 - main_output_loss: 0.4975 - aux_output_loss: 1.2424 - val_loss: 0.7347 - val_main_output_loss: 0.4830 - val_aux_output_loss: 3.0003\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5562 - main_output_loss: 0.4853 - aux_output_loss: 1.1939 - val_loss: 0.6839 - val_main_output_loss: 0.4653 - val_aux_output_loss: 2.6508\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5435 - main_output_loss: 0.4759 - aux_output_loss: 1.1516 - val_loss: 0.6434 - val_main_output_loss: 0.4583 - val_aux_output_loss: 2.3090\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5330 - main_output_loss: 0.4686 - aux_output_loss: 1.1126 - val_loss: 0.6042 - val_main_output_loss: 0.4401 - val_aux_output_loss: 2.0808\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5239 - main_output_loss: 0.4622 - aux_output_loss: 1.0791 - val_loss: 0.5762 - val_main_output_loss: 0.4344 - val_aux_output_loss: 1.8523\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5159 - main_output_loss: 0.4569 - aux_output_loss: 1.0474 - val_loss: 0.5538 - val_main_output_loss: 0.4241 - val_aux_output_loss: 1.7213\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5088 - main_output_loss: 0.4523 - aux_output_loss: 1.0174 - val_loss: 0.5368 - val_main_output_loss: 0.4185 - val_aux_output_loss: 1.6021\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5027 - main_output_loss: 0.4484 - aux_output_loss: 0.9907 - val_loss: 0.5224 - val_main_output_loss: 0.4121 - val_aux_output_loss: 1.5152\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4971 - main_output_loss: 0.4449 - aux_output_loss: 0.9671 - val_loss: 0.5094 - val_main_output_loss: 0.4084 - val_aux_output_loss: 1.4187\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4923 - main_output_loss: 0.4420 - aux_output_loss: 0.9444 - val_loss: 0.4995 - val_main_output_loss: 0.4060 - val_aux_output_loss: 1.3413\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4873 - main_output_loss: 0.4389 - aux_output_loss: 0.9229 - val_loss: 0.4892 - val_main_output_loss: 0.4028 - val_aux_output_loss: 1.2671\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4825 - main_output_loss: 0.4358 - aux_output_loss: 0.9029 - val_loss: 0.4809 - val_main_output_loss: 0.4013 - val_aux_output_loss: 1.1977\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4789 - main_output_loss: 0.4339 - aux_output_loss: 0.8843 - val_loss: 0.4778 - val_main_output_loss: 0.4030 - val_aux_output_loss: 1.1506\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4703 - main_output_loss: 0.4270 - aux_output_loss: 0.8596\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto la `API Sequential` como la `API Funtional`  son declarativas: se empieza por declarar qué capas se desean utilizar y cómo deben conectar, y sólo entonces se puede empezar a alimentar el modelo con\n",
    "algunos datos para el entrenamiento o la inferencia. Esto tiene muchas\n",
    "ventajas: el modelo puede guardarse, clonarse y compartirse fácilmente; su\n",
    "estructura puede visualizarse y analizarse; el marco puede inferir formas y\n",
    "comprobar tipos, por lo que los errores pueden detectarse pronto (es decir,\n",
    "antes de que ningún dato pase por el modelo). También es bastante fácil de\n",
    "depurar, ya que todo el modelo es un gráfico estático de capas. Pero la otra\n",
    "cara de la moneda es simplemente eso: es estático.\n",
    " \n",
    "Algunos modelos implican bucles, formas\n",
    "variables, bifurcaciones condicionales y otros comportamientos dinámicos.\n",
    "Para estos casos, o simplemente si prefiere un estilo de programación\n",
    "más imperativo, la `Subclassing API` es para usted. Basta con subclasificar la clase Model, crear las capas que necesite en elconstructor y utilizarlas para realizar los cálculos que desee en el método\n",
    "`call()`. \n",
    "\n",
    "\n",
    "Esta flexibilidad extra tiene un coste: la arquitectura de tu modelo está\n",
    "oculta dentro del método call(), por lo que Keras no puede inspeccionarlo\n",
    "fácilmente; no puede guardarlo o clonarlo; y cuando llamas al método\n",
    "summary(), sólo obtienes una lista de capas, sin ninguna información sobre\n",
    "cómo están conectadas entre sí. Además, Keras no puede comprobar los\n",
    "tipos y formas de antemano, y por lo tanto mas fácil cometer errores\n",
    "___\n",
    " Por ejemplo, creando una instancia de la siguiente clase\n",
    "**WideAndDeepModel** obtenemos un modelo equivalente al que acabamos\n",
    "de construir con la API Funcional. A continuación, puede compilarlo,\n",
    "evaluarlo y utilizarlo para hacer predicciones, exactamente como\n",
    "acabamos de hacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = Dense(units, activation=activation)\n",
    "        self.hidden2 = Dense(units, activation=activation)\n",
    "        self.main_output = Dense(1)\n",
    "        self.aux_output = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 2.9256 - output_1_loss: 2.6530 - output_2_loss: 5.3792 - val_loss: 1.8043 - val_output_1_loss: 1.2635 - val_output_2_loss: 6.6716\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.1196 - output_1_loss: 0.8814 - output_2_loss: 3.2635 - val_loss: 1.1044 - val_output_1_loss: 0.8003 - val_output_2_loss: 3.8410\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.8413 - output_1_loss: 0.6978 - output_2_loss: 2.1334 - val_loss: 0.8301 - val_output_1_loss: 0.6333 - val_output_2_loss: 2.6013\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7286 - output_1_loss: 0.6304 - output_2_loss: 1.6125 - val_loss: 0.7218 - val_output_1_loss: 0.5825 - val_output_2_loss: 1.9749\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6666 - output_1_loss: 0.5894 - output_2_loss: 1.3617 - val_loss: 0.6550 - val_output_1_loss: 0.5440 - val_output_2_loss: 1.6549\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6260 - output_1_loss: 0.5582 - output_2_loss: 1.2361 - val_loss: 0.6128 - val_output_1_loss: 0.5167 - val_output_2_loss: 1.4778\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 2s 7ms/step - loss: 0.5953 - output_1_loss: 0.5328 - output_2_loss: 1.1576 - val_loss: 0.5828 - val_output_1_loss: 0.4963 - val_output_2_loss: 1.3614\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5725 - output_1_loss: 0.5132 - output_2_loss: 1.1059 - val_loss: 0.5524 - val_output_1_loss: 0.4743 - val_output_2_loss: 1.2555\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5536 - output_1_loss: 0.4966 - output_2_loss: 1.0664 - val_loss: 0.5319 - val_output_1_loss: 0.4595 - val_output_2_loss: 1.1833\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5385 - output_1_loss: 0.4836 - output_2_loss: 1.0333 - val_loss: 0.5158 - val_output_1_loss: 0.4474 - val_output_2_loss: 1.1316\n",
      "162/162 [==============================] - 1s 3ms/step - loss: 0.5200 - output_1_loss: 0.4664 - output_2_loss: 1.0028\n",
      "1/1 [==============================] - 0s 189ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se utiliza la `API Secuencial` o la `API Funcional`, guardar un\n",
    "modelo Keras entrenado es de lo más sencillo:\n",
    "```python \n",
    "    model = keras.layers.Sequential([...]) # o keras.Model([...])\n",
    "    model.compile([...])\n",
    "    model.fit([...])\n",
    "    model.save(\"mi_modelo_keras.h5\")\n",
    "    \n",
    "    #cargar en otro scripts\n",
    "    keras.models.load_model(\"mi_modelo_keras.h5\")\n",
    "```\n",
    "\n",
    "Keras utilizará el formato HDF5 para guardar tanto la arquitectura del\n",
    "modelo (incluyendo los hiperparámetros de cada capa) como los valores\n",
    "de todos los parámetros del modelo para cada capa (por ejemplo, pesos\n",
    "de conexión y sesgos). También guarda el optimizador (incluyendo sus\n",
    "hiperparámetros y cualquier estado que pueda tener).\n",
    " \n",
    "Desafortunadamente  cuando se utilice la `Subclassing`  no se puede guardar fácilmente el modelo, pero se puede utilizar **save_weights()** y **load_weights()** para al menos guardar y restaurar los parámetros del modelo, pero tendrá que guardar y restaurar todo lo demás usted mismo\n",
    "```python \n",
    "    model.save_weights(\"my_keras_weights.ckpt\")\n",
    "    model.load_weights(\"my_keras_weights.ckpt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 1.9701 - val_loss: 0.7871\n",
      "Epoch 2/3\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.7556 - val_loss: 0.8961\n",
      "Epoch 3/3\n",
      "363/363 [==============================] - 3s 7ms/step - loss: 0.6958 - val_loss: 0.7924\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    Dense(30, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"resources/my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import  load_model\n",
    "model_restaured = load_model(\"resources/my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9575483]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_restaured.predict(X_test[:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Callbacks during Training(for examle: *EarlyStop*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método fit() acepta un argumento `callbacks` que te permite especificar\n",
    "una lista de objetos que Keras llamará al inicio y al final del\n",
    "entrenamiento, al inicio y al final de cada epoch, e incluso antes y\n",
    "después de procesar cada lote. Por ejemplo, el callback **ModelCheckpoint**\n",
    "guarda puntos de control de tu modelo a intervalos regulares durante el\n",
    "entrenamiento, por defecto al final de cada epoch.\n",
    "\n",
    "Si utiliza un conjunto de validación durante el entrenamiento,\n",
    "puede establecer **save_best_only=True** al crear el **ModelCheckpoint**. En\n",
    "este caso, sólo guardará su modelo cuando su rendimiento en el\n",
    "conjunto de validación sea el mejor hasta el momento. De esta forma, no\n",
    "tendrá que preocuparse de entrenar durante demasiado tiempo y\n",
    "sobreajustar el conjunto de entrenamiento: simplemente restaure el último\n",
    "modelo guardado después del entrenamiento, y éste será el mejor modelo\n",
    "en el conjunto de validación. El siguiente código es una forma sencilla de\n",
    "aplicar `Early Stopping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3261 - val_loss: 0.3179\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3261 - val_loss: 0.3170\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3255 - val_loss: 0.3203\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3258 - val_loss: 0.3483\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3256 - val_loss: 0.3324\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3251 - val_loss: 0.3242\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3247 - val_loss: 0.3122\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3242 - val_loss: 0.3142\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3239 - val_loss: 0.3173\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3237 - val_loss: 0.3107\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3288\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\"resources/my_keras_model_early_stop.h5\", save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = load_model(\"resources/my_keras_model_early_stop.h5\")  # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de aplicar `Early Stopping` es utilizar simplemente la callback EarlyStopping. Interrumpirá el entrenamiento cuando no mida ningún progreso en el conjunto de validación durante\n",
    "un número de épocas (definido por el argumento paciencia), y opcionalmente volverá al mejor modelo.\n",
    " \n",
    "En este caso, no hay necesidad de restaurar el mejor modelo\n",
    "guardado porque la llamada de retorno EarlyStopping mantendrá un\n",
    "registro de los mejores pesos y los restaurará por ti al final del\n",
    "entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3339 - val_loss: 0.3316\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3338 - val_loss: 0.3466\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3332 - val_loss: 0.3311\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3331 - val_loss: 0.3217\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3326 - val_loss: 0.3223\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3317 - val_loss: 0.3184\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3315 - val_loss: 0.3153\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3308 - val_loss: 0.3176\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3304 - val_loss: 0.3182\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3299 - val_loss: 0.3269\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3297 - val_loss: 0.3178\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3293 - val_loss: 0.3565\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3287 - val_loss: 0.3144\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3284 - val_loss: 0.3132\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3279 - val_loss: 0.3156\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3277 - val_loss: 0.3373\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3275 - val_loss: 0.3632\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3274 - val_loss: 0.3115\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3266 - val_loss: 0.3114\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3265 - val_loss: 0.3421\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3262 - val_loss: 0.3148\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3256 - val_loss: 0.3717\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3258 - val_loss: 0.3717\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3253 - val_loss: 0.3526\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3251 - val_loss: 0.3127\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3242 - val_loss: 0.3314\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3243 - val_loss: 0.3703\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3243 - val_loss: 0.3638\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3241 - val_loss: 0.3120\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3313\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "early_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history.history['loss'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si necesita un control adicional(por ejemplo aumentar learning_rate), puede escribir fácilmente sus propias\n",
    "callbacks. Como ejemplo de cómo hacerlo, la\n",
    "siguiente llamada de retorno personalizada mostrará la relación entre la\n",
    "pérdida de validación y la pérdida de entrenamiento durante el\n",
    "entrenamiento (por ejemplo, para detectar el sobreajuste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/363 [==========================>...] - ETA: 0s - loss: 0.3211\n",
      "val/train: 0.96\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3222 - val_loss: 0.3092\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento debe implementar **on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(),on_batch_begin(), y on_batch_end()**. \n",
    "\n",
    " Para la evaluación, debe implementar **on_test_begin(), on_test_end(),on_test_batch_begin(), o on_test_batch_end()** (llamado por evaluate()), \n",
    " \n",
    "Para la predicción debe implementar\n",
    "**on_predict_begin(),on_predict_end(), on_predict_batch_begin(), o on_predict_batch_end()**\n",
    "(llamado por predict())."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
